<?xml version="1.0"?>
<config_machines version="2.0">
  <machine MACH="levante">
    <DESC>BullSequana XH2000 with AMD EPYC CPUs (Milan), os is Linux, 128 pes/node, batch system is Slurm</DESC>
    <NODENAME_REGEX>^levante.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>aa0049</PROJECT>
    <CHARGE_ACCOUNT>aa0049</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR> </SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS> </SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/scratch/a/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/work/aa0049/a271098/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/work/aa0049/a271098/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/work/aa0049/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/work/aa0049/a271098/CTSM/cime/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work/aa0049/a271098/CTSM/cime/tools/cprnc</CCSM_CPRNC>
    <GMAKE>gmake</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>awi</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label">-l</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }} </arg>
        <arg name="cpu_bind">--cpu_bind=verbose,cores</arg>
        <arg name="distribution">--distribution=block:cyclic</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <cmd_path lang="python">/usr/share/Modules/libexec/modulecmd.tcl python</cmd_path>
      <cmd_path lang="perl">/usr/share/Modules/libexec/modulecmd.tcl perl</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules compiler="intel">
        <command name="load">git/2.31.1-gcc-11.2.0</command>
        <command name="load">python3/2022.01-gcc-11.2.0</command>
        <command name="load">perl/5.34.0-gcc-11.2.0</command>
        <command name="load">intel-oneapi-compilers/2022.0.1-gcc-11.2.0</command>
        <command name="load">intel-oneapi-mkl/2022.0.1-gcc-11.2.0</command>
        <command name="load">esmf/8.2.0-intel-2021.5.0</command>
      </modules>
      <modules compiler="gcc">
        <command name="load">python3/2022.01-gcc-11.2.0</command>
        <command name="load">perl/5.34.0-gcc-11.2.0</command>
        <command name="load">gcc/11.2.0-gcc-11.2.0</command>
        <command name="load">esmf/8.2.0-gcc-11.2.0</command>
      </modules>
      <modules compiler="intel" mpilib="openmpi">
        <command name="load">openmpi/4.1.2-intel-2021.5.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="NETCDF_C_PATH">/sw/spack-levante/netcdf-c-4.8.1-2k3cmu</env>
      <env name="NETCDF_FORTRAN_PATH">/sw/spack-levante/netcdf-fortran-4.5.3-k6xq5g</env>
    </environment_variables>
    <environment_variables comp_interface="nuopc">
      <env name="ESMFMKFILE">/sw/spack-levante/esmf-8.2.0-zlkzvy/lib/esmf.mk</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>
</config_machines>
